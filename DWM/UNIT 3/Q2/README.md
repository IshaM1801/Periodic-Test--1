ðŸ“˜ Basic Algorithm (Naive Bayes Classifier â€“ Probabilistic Model)

1. Start with the Training Dataset
   â€¢ Each data sample has features X = (x_1, x_2, â€¦, x_n) and a class label C_k.
   â€¢ Goal: Predict the class of a new instance based on feature values.

â¸»

2. Calculate Prior Probabilities
   â€¢ Compute the prior probability for each class:
   P(C_k) = \frac{\text{Number of training samples in } C_k}{\text{Total number of samples}}

â¸»

3. Compute Conditional Probabilities (Likelihood)
   â€¢ For each feature x_i, calculate the likelihood given a class:
   P(x_i | C_k)
   â€¢ For categorical features, use frequency counts.
   â€¢ For continuous features, assume Gaussian distribution and use:
   P(x_i | C_k) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(x_i - \mu)^2}{2\sigma^2}}

â¸»

4. Apply Naive Bayes Formula
   â€¢ Compute the posterior for each class using Bayesâ€™ Theorem:
   P(C*k | X) \propto P(C_k) \cdot \prod*{i=1}^{n} P(x_i | C_k)
   â€¢ Ignore the denominator as itâ€™s constant across classes.

â¸»

5. Make the Final Classification
   â€¢ Choose the class with the maximum posterior probability:
   \text{Predicted Class} = \arg\max*{C_k} P(C_k) \cdot \prod*{i=1}^{n} P(x_i | C_k)

â¸»

âœ… Assumptions and Characteristics
â€¢ Naive Assumption: All features are conditionally independent given the class.
â€¢ Efficient: Works well with high-dimensional data (e.g., text classification).
â€¢ Limitations: Independence assumption may not hold true in all datasets.

â¸»
